{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3f85ff48",
      "metadata": {
        "id": "3f85ff48"
      },
      "source": [
        "# Modelo Quantizado para Reconhecimento de Anel de Sinete\n",
        "\n",
        "Este notebook gera um modelo quantizado para classificação de tecido com ou sem anel de sinete. Tal modelo é utilizado na geração do mapa de calor para a WSI.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A biblioteca a seguir não é padrão do google colab, por isso é necessário\n",
        "# instala-la antes de executar a célula de importação das bibliotecas\n",
        "!pip install tensorflow_model_optimization"
      ],
      "metadata": {
        "id": "eGt7V65SietY"
      },
      "id": "eGt7V65SietY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7623967f",
      "metadata": {
        "id": "7623967f"
      },
      "outputs": [],
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "import os\n",
        "import pathlib\n",
        "import warnings\n",
        "import platform\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras import layers, metrics\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "30a7ac59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30a7ac59",
        "outputId": "0566a0a9-c8d1-409c-d361-636bd93f18b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python- Versão:  3.10.12\n",
            "TensorFLow - Versão:  2.12.0\n",
            "Dispositivo GPU padrão: /device:GPU:0\n",
            "Número de GPUs Disponíveis:  1\n"
          ]
        }
      ],
      "source": [
        "print(\"Python- Versão: \", platform.python_version())\n",
        "print(\"TensorFLow - Versão: \",tf.__version__)\n",
        "\n",
        "# Checar GPUs\n",
        "if not tf.test.gpu_device_name():\n",
        "    warnings.warn('GPU não encontrada.')\n",
        "else:\n",
        "    print(f'Dispositivo GPU padrão: {tf.test.gpu_device_name()}')\n",
        "    print(\"Número de GPUs Disponíveis: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aIHIOsMizEW",
        "outputId": "7b2dd95d-6dd8-44c6-eabd-b49cd5245e0f"
      },
      "id": "_aIHIOsMizEW",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extrai o dataset de anel de sinete\n",
        "!tar -xzvf \"/content/drive/MyDrive/1_ DESENVOLVIMENTO  AMPLIAR TEC 4.0/Datasets/Anel de Sinete/TCGA-BR-8291-dataset-sinet-ring.tar.gz\""
      ],
      "metadata": {
        "id": "zH4UO776jCr2"
      },
      "id": "zH4UO776jCr2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b2b164b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2b164b9",
        "outputId": "38578c16-28d1-4076-ff32-f14dc3f18f7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existem 2760 imagens em  dataset-sinet-ring/train/com anel.\n",
            "Existem 3630 imagens em  dataset-sinet-ring/train/sem anel.\n",
            "Existem 86 imagens em  dataset-sinet-ring/val/com anel.\n",
            "Existem 90 imagens em  dataset-sinet-ring/val/sem anel.\n",
            "Existem 86 imagens em  dataset-sinet-ring/test/com anel.\n",
            "Existem 90 imagens em  dataset-sinet-ring/test/sem anel.\n"
          ]
        }
      ],
      "source": [
        "for dirpath, dirnames, filenames in os.walk('dataset-sinet-ring'):\n",
        "    if len(filenames)!=0:\n",
        "        print(f'Existem {len(filenames)} imagens em  {dirpath}.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b9ebe3fe",
      "metadata": {
        "id": "b9ebe3fe"
      },
      "outputs": [],
      "source": [
        "train_dir = 'dataset-sinet-ring/train'\n",
        "val_dir = 'dataset-sinet-ring/val'\n",
        "test_dir = 'dataset-sinet-ring/test'\n",
        "\n",
        "# Especificações para o treinamento da rede\n",
        "IMG_SHAPE = (224, 224, 3)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "37e2ebd5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37e2ebd5",
        "outputId": "ad05772b-b530-4af7-9e3c-5dd0689bfb9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6390 images belonging to 2 classes.\n",
            "Found 176 images belonging to 2 classes.\n",
            "Found 176 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_datagen = ImageDataGenerator(rescale=None)\n",
        "valid_datagen = ImageDataGenerator(rescale=None)\n",
        "test_datagen = ImageDataGenerator(rescale=None)\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(train_dir,\n",
        "                                               batch_size=BATCH_SIZE,\n",
        "                                               target_size=(224, 224),\n",
        "                                               class_mode=\"categorical\")\n",
        "\n",
        "val_data = valid_datagen.flow_from_directory(val_dir,\n",
        "                                               batch_size=BATCH_SIZE,\n",
        "                                               target_size=(224, 224),\n",
        "                                               class_mode=\"categorical\")\n",
        "\n",
        "test_data = test_datagen.flow_from_directory(test_dir,\n",
        "                                               batch_size=BATCH_SIZE,\n",
        "                                               target_size=(224, 224),\n",
        "                                               class_mode=\"categorical\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QW5ujFL5g9B6"
      },
      "outputs": [],
      "source": [
        "# Criação do modelo\n",
        "\n",
        "base_model = tf.keras.applications.MobileNetV2(include_top=True,\n",
        "                                               input_shape=IMG_SHAPE,\n",
        "                                               weights='imagenet',\n",
        "                                               pooling='max')\n",
        "\n",
        "# descongela as duas últimas camadas do modelo\n",
        "base_output = base_model.layers[-2]\n",
        "\n",
        "outputs = tf.keras.layers.Dense(2, activation='softmax')(base_output.output)\n",
        "\n",
        "qat_model = tf.keras.models.Model(inputs=base_model.inputs, outputs=outputs)\n",
        "\n",
        "qat_model = tfmot.quantization.keras.quantize_model(qat_model)"
      ],
      "id": "QW5ujFL5g9B6"
    },
    {
      "cell_type": "markdown",
      "id": "a1268af2",
      "metadata": {
        "id": "a1268af2"
      },
      "source": [
        "## Fine Tuning - Fase 1 (warm up) -> treina apenas a camada densa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "fa58a144",
      "metadata": {
        "id": "fa58a144"
      },
      "outputs": [],
      "source": [
        "# Congela as camadas do modelo base (as camadas de convolução não serão treinadas novamente)\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "e0bb1d43",
      "metadata": {
        "id": "e0bb1d43"
      },
      "outputs": [],
      "source": [
        "base_learning_rate = 0.0001\n",
        "opt = Adam(learning_rate=base_learning_rate)\n",
        "# binary -> binary_crossentropy; categorical -> categorical_crossentropy\n",
        "metrics = ['acc', tf.keras.metrics.AUC()]\n",
        "qat_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=opt, metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "e3e112d2",
      "metadata": {
        "id": "e3e112d2"
      },
      "outputs": [],
      "source": [
        "# Early Stopping do trinamento\n",
        "early_stop = keras.callbacks.EarlyStopping(\n",
        "    monitor = 'val_loss',\n",
        "    min_delta=0.001,\n",
        "    patience=7\n",
        ")\n",
        "\n",
        "model_ckpt = keras.callbacks.ModelCheckpoint(\n",
        "    filepath='callbacks/melhor_modelo_MobileNetV2_ft.hdf5',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True\n",
        ")\n",
        "\n",
        "callbacks = [early_stop, model_ckpt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "274eb4d3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "274eb4d3",
        "outputId": "7f53c96c-336b-4c25-9394-df8d128a0b02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "200/200 [==============================] - 137s 425ms/step - loss: 0.2939 - acc: 0.8684 - auc: 0.9474 - val_loss: 1.1307 - val_acc: 0.5568 - val_auc: 0.5547\n",
            "Epoch 2/40\n",
            "200/200 [==============================] - 75s 376ms/step - loss: 0.1159 - acc: 0.9559 - auc: 0.9919 - val_loss: 2.0988 - val_acc: 0.5568 - val_auc: 0.5959\n",
            "Epoch 3/40\n",
            "200/200 [==============================] - 79s 397ms/step - loss: 0.0610 - acc: 0.9770 - auc: 0.9978 - val_loss: 4.6464 - val_acc: 0.5000 - val_auc: 0.5142\n",
            "Epoch 4/40\n",
            "200/200 [==============================] - 77s 383ms/step - loss: 0.0278 - acc: 0.9914 - auc: 0.9996 - val_loss: 3.5558 - val_acc: 0.5170 - val_auc: 0.5719\n",
            "Epoch 5/40\n",
            "200/200 [==============================] - 75s 377ms/step - loss: 0.0268 - acc: 0.9906 - auc: 0.9995 - val_loss: 1.6003 - val_acc: 0.6648 - val_auc: 0.7314\n",
            "Epoch 6/40\n",
            "200/200 [==============================] - 75s 376ms/step - loss: 0.0149 - acc: 0.9948 - auc: 0.9999 - val_loss: 5.8998 - val_acc: 0.5341 - val_auc: 0.5418\n",
            "Epoch 7/40\n",
            "200/200 [==============================] - 74s 372ms/step - loss: 0.0229 - acc: 0.9922 - auc: 0.9992 - val_loss: 3.1894 - val_acc: 0.6193 - val_auc: 0.6830\n",
            "Epoch 8/40\n",
            "200/200 [==============================] - 76s 380ms/step - loss: 0.0197 - acc: 0.9928 - auc: 0.9998 - val_loss: 0.8919 - val_acc: 0.8239 - val_auc: 0.8892\n",
            "Epoch 9/40\n",
            "200/200 [==============================] - 77s 385ms/step - loss: 0.0114 - acc: 0.9969 - auc: 0.9998 - val_loss: 1.3241 - val_acc: 0.8125 - val_auc: 0.8742\n",
            "Epoch 10/40\n",
            "200/200 [==============================] - 79s 393ms/step - loss: 0.0211 - acc: 0.9930 - auc: 0.9993 - val_loss: 1.5936 - val_acc: 0.8409 - val_auc: 0.8811\n",
            "Epoch 11/40\n",
            "200/200 [==============================] - 76s 379ms/step - loss: 0.0146 - acc: 0.9953 - auc: 0.9999 - val_loss: 2.5739 - val_acc: 0.7955 - val_auc: 0.8255\n",
            "Epoch 12/40\n",
            "200/200 [==============================] - 76s 377ms/step - loss: 0.0127 - acc: 0.9964 - auc: 0.9996 - val_loss: 3.5645 - val_acc: 0.7330 - val_auc: 0.7685\n",
            "Epoch 13/40\n",
            "200/200 [==============================] - 74s 369ms/step - loss: 0.0077 - acc: 0.9977 - auc: 1.0000 - val_loss: 2.1203 - val_acc: 0.8125 - val_auc: 0.8521\n",
            "Epoch 14/40\n",
            "200/200 [==============================] - 75s 375ms/step - loss: 0.0127 - acc: 0.9956 - auc: 0.9998 - val_loss: 0.5590 - val_acc: 0.8977 - val_auc: 0.9552\n",
            "Epoch 15/40\n",
            "200/200 [==============================] - 75s 374ms/step - loss: 0.0057 - acc: 0.9984 - auc: 1.0000 - val_loss: 0.4188 - val_acc: 0.9261 - val_auc: 0.9600\n",
            "Epoch 16/40\n",
            "200/200 [==============================] - 74s 370ms/step - loss: 0.0048 - acc: 0.9987 - auc: 1.0000 - val_loss: 0.6262 - val_acc: 0.8977 - val_auc: 0.9520\n",
            "Epoch 17/40\n",
            "200/200 [==============================] - 74s 371ms/step - loss: 0.0053 - acc: 0.9983 - auc: 1.0000 - val_loss: 0.4996 - val_acc: 0.9205 - val_auc: 0.9519\n",
            "Epoch 18/40\n",
            "200/200 [==============================] - 74s 371ms/step - loss: 0.0105 - acc: 0.9962 - auc: 0.9999 - val_loss: 3.6978 - val_acc: 0.6591 - val_auc: 0.7066\n",
            "Epoch 19/40\n",
            "200/200 [==============================] - 78s 390ms/step - loss: 0.0129 - acc: 0.9958 - auc: 0.9996 - val_loss: 3.0412 - val_acc: 0.7443 - val_auc: 0.7676\n",
            "Epoch 20/40\n",
            "200/200 [==============================] - 81s 402ms/step - loss: 0.0189 - acc: 0.9931 - auc: 0.9995 - val_loss: 0.6853 - val_acc: 0.8693 - val_auc: 0.9369\n",
            "Epoch 21/40\n",
            "200/200 [==============================] - 74s 370ms/step - loss: 0.0049 - acc: 0.9989 - auc: 1.0000 - val_loss: 0.4655 - val_acc: 0.9261 - val_auc: 0.9599\n",
            "Epoch 22/40\n",
            "200/200 [==============================] - 75s 373ms/step - loss: 0.0085 - acc: 0.9980 - auc: 0.9997 - val_loss: 0.5589 - val_acc: 0.9205 - val_auc: 0.9534\n"
          ]
        }
      ],
      "source": [
        "# Fase 1 - treina apenas com algumas épocas (40% do total)\n",
        "initial_epochs = int(EPOCHS*0.4)\n",
        "history = qat_model.fit(train_data,\n",
        "                    epochs=initial_epochs,\n",
        "                    steps_per_epoch=len(train_data),\n",
        "                    validation_data=val_data,\n",
        "                    validation_steps=len(val_data),\n",
        "                    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9db89e4d",
      "metadata": {
        "id": "9db89e4d"
      },
      "source": [
        "## Fine Tuning - Fase 2 -> treina todo o modelo do fine tuning - descongela as camadas de convolução"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec494850",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec494850",
        "outputId": "046f6da1-5a10-402e-be0a-851400a765cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de camadas do modelo base:  158\n"
          ]
        }
      ],
      "source": [
        "print(\"Número de camadas do modelo base: \", len(basemodel.layers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a0301315",
      "metadata": {
        "id": "a0301315"
      },
      "outputs": [],
      "source": [
        "# Descongelar as 30% camadas finais e que não seja do tipo BatchNormalization do modelo base\n",
        "# Note que vpcê pode adicionar mais camadas se o modelo não for muito grande\n",
        "count = 0\n",
        "for layer in base_model.layers[-int(len(base_model.layers)*0.3):]:\n",
        "    if not isinstance(layer, layers.BatchNormalization):\n",
        "        layer.trainable = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "ba855c0d",
      "metadata": {
        "id": "ba855c0d"
      },
      "outputs": [],
      "source": [
        "# É necessário diminuir a base_learning_rate para que o modelo não tenha convergência muito rápida\n",
        "# pois se trata d eum modelo grande\n",
        "opt = Adam(learning_rate=base_learning_rate/10)\n",
        "# binary -> binary_crossentropy; categorical -> categorical_crossentropy\n",
        "qat_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=opt, metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "50ad7d61",
      "metadata": {
        "id": "50ad7d61"
      },
      "outputs": [],
      "source": [
        "# EarlyStopping do trinamento\n",
        "early_stop2 = keras.callbacks.EarlyStopping(\n",
        "    monitor = 'val_loss',\n",
        "    min_delta=0.001,\n",
        "    patience=7\n",
        ")\n",
        "\n",
        "model_ckpt2 = keras.callbacks.ModelCheckpoint(\n",
        "    filepath='callbacks/melhor_modelo_MobileNetV2_ft.hdf5',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True\n",
        ")\n",
        "\n",
        "callbacks2 = [early_stop2, model_ckpt2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "788b3350",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "788b3350",
        "outputId": "2d00e4b1-3871-434a-86f0-1a868bbf3334"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/100\n",
            "200/200 [==============================] - 112s 386ms/step - loss: 0.0026 - acc: 0.9995 - auc: 0.9989 - val_loss: 0.5937 - val_acc: 0.9148 - val_auc: 0.9413\n",
            "Epoch 23/100\n",
            "200/200 [==============================] - 77s 383ms/step - loss: 0.0043 - acc: 0.9989 - auc: 0.9998 - val_loss: 0.4775 - val_acc: 0.9261 - val_auc: 0.9522\n",
            "Epoch 24/100\n",
            "200/200 [==============================] - 75s 375ms/step - loss: 0.0020 - acc: 0.9994 - auc: 1.0000 - val_loss: 0.4140 - val_acc: 0.9318 - val_auc: 0.9572\n",
            "Epoch 25/100\n",
            "200/200 [==============================] - 76s 380ms/step - loss: 0.0023 - acc: 0.9994 - auc: 1.0000 - val_loss: 0.3548 - val_acc: 0.9375 - val_auc: 0.9589\n",
            "Epoch 26/100\n",
            "200/200 [==============================] - 77s 382ms/step - loss: 0.0020 - acc: 0.9994 - auc: 1.0000 - val_loss: 0.3543 - val_acc: 0.9375 - val_auc: 0.9641\n",
            "Epoch 27/100\n",
            "200/200 [==============================] - 76s 379ms/step - loss: 9.1971e-04 - acc: 0.9998 - auc: 1.0000 - val_loss: 0.3757 - val_acc: 0.9261 - val_auc: 0.9580\n",
            "Epoch 28/100\n",
            "200/200 [==============================] - 76s 378ms/step - loss: 7.9406e-04 - acc: 0.9998 - auc: 1.0000 - val_loss: 0.3568 - val_acc: 0.9318 - val_auc: 0.9534\n",
            "Epoch 29/100\n",
            "200/200 [==============================] - 75s 374ms/step - loss: 6.0593e-04 - acc: 0.9997 - auc: 1.0000 - val_loss: 0.3579 - val_acc: 0.9375 - val_auc: 0.9587\n",
            "Epoch 30/100\n",
            "200/200 [==============================] - 75s 375ms/step - loss: 0.0013 - acc: 0.9995 - auc: 1.0000 - val_loss: 0.3563 - val_acc: 0.9318 - val_auc: 0.9680\n",
            "Epoch 31/100\n",
            "200/200 [==============================] - 75s 376ms/step - loss: 0.0010 - acc: 0.9995 - auc: 1.0000 - val_loss: 0.3890 - val_acc: 0.9261 - val_auc: 0.9564\n",
            "Epoch 32/100\n",
            "200/200 [==============================] - 76s 381ms/step - loss: 7.3172e-04 - acc: 0.9997 - auc: 1.0000 - val_loss: 0.3818 - val_acc: 0.9318 - val_auc: 0.9578\n"
          ]
        }
      ],
      "source": [
        "# Fase 2 - treina apenas com a quantidade de épocas que faltam (60% finais)\n",
        "# Note que iniciará de initial_epoch=history.epoch[-1]\n",
        "history_fine = qat_model.fit(train_data,\n",
        "                    epochs=EPOCHS,\n",
        "                    initial_epoch=history.epoch[-1],\n",
        "                    steps_per_epoch=len(train_data),\n",
        "                    validation_data=val_data,\n",
        "                    validation_steps=len(val_data),\n",
        "                    callbacks=callbacks2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(qat_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "quantized_tflite_model = converter.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fw_Op5g26shx",
        "outputId": "d9ae7372-0c5c-414a-ff23-d5464eba1c2e"
      },
      "id": "Fw_Op5g26shx",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as Conv1_layer_call_fn, Conv1_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, Conv1_relu_layer_call_fn, Conv1_relu_layer_call_and_return_conditional_losses while saving (showing 5 of 248). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py:789: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "DYJgAuZqg9B8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c29933e1-a161-4d87-f575-5b95dbeeab83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2710576"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# salvando modelo quantizado\n",
        "tflite_model_name = 'TCGA-BR-8291_qat_sinete_model_1.tflite'\n",
        "open(tflite_model_name, \"wb\").write(quantized_tflite_model)"
      ],
      "id": "DYJgAuZqg9B8"
    },
    {
      "cell_type": "code",
      "source": [
        "!mv TCGA-CD-A489_qat_sinete_model_1.tflite '/content/drive/MyDrive/1_ DESENVOLVIMENTO  AMPLIAR TEC 4.0/Notebooks DL/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzDwkyBa8CYJ",
        "outputId": "ab15cbcb-db2d-4150-c61c-b3f8ce2e8337"
      },
      "id": "IzDwkyBa8CYJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot overwrite directory '/content/drive/MyDrive/1_ DESENVOLVIMENTO  AMPLIAR TEC 4.0/Notebooks DL/TCGA-CD-A489_qat_sinete_model_1.tflite' with non-directory\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}